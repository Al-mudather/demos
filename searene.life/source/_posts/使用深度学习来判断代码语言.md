---
title: 使用深度学习来判断代码语言
date: 2017-11-26 15:56:45
tags: ['神经网络']
thumbnail: /images/deep-learning.jpg
categories: Coding
---

# 介绍

本篇文章介绍了如何使用深度学习来判断代码语言。以下面这段代码为例。

```python
def test():
    print("something")
```

如果用本篇文章所介绍的程序来判断这段代码的话，我们得到的答案就是`python`，这也是正确答案。事实上，通过初步测试，该程度的准确度在90%左右。相信通过增加训练数据、参数微调等方式，我们还可以得到更加理想的结果。

# 运行

我们首先尝试运行一下程序，以便对本篇文章的成果有一个直观的感受。

1. 安装第三方库

   - [Anaconda(Python 3.6+版本)](https://www.anaconda.com/download/)

   - Gensim

     ```bash
     conda install -c anaconda gensim
     ```

   - Keras

     ```bash
     conda install -c conda-forge keras
     ```

   -  Tensorflow

     ```bash
     pip install tensorflow==1.3.0
     ```

2. 下载程序

  ```bash
  git clone git@github.com:searene/demos.git && cd demos/PLDetector-demo
  ```

3. 训练模型

   ```bash
   python -m src.neural_network_trainer                                                  
   Using TensorFlow backend.                                                                                             
   ...
   _________________________________________________________________
   Layer (type)                 Output Shape              Param #
   =================================================================
   embedding_1 (Embedding)      (None, 500, 100)          773100
   _________________________________________________________________
   conv1d_1 (Conv1D)            (None, 496, 128)          64128                                                          
   _________________________________________________________________                                                     
   max_pooling1d_1 (MaxPooling1 (None, 248, 128)          0                                                              
   _________________________________________________________________                                                     
   flatten_1 (Flatten)          (None, 31744)             0                                                              
   _________________________________________________________________                                                     
   dense_1 (Dense)              (None, 8)                 253960                                                         
   =================================================================                                                     
   Total params: 1,091,188                                                                                               
   Trainable params: 318,088
   Non-trainable params: 773,100
   _________________________________________________________________
   INFO:root:None
   Epoch 1/10
    - 1s - loss: 0.4304 - acc: 0.8823
   Epoch 2/10
    - 1s - loss: 0.1357 - acc: 0.9657
   Epoch 3/10
    - 1s - loss: 0.0706 - acc: 0.9788
   Epoch 4/10
    - 1s - loss: 0.0392 - acc: 0.9887
   Epoch 5/10
    - 1s - loss: 0.0266 - acc: 0.9927
   Epoch 6/10
    - 1s - loss: 0.0203 - acc: 0.9945
   Epoch 7/10
    - 1s - loss: 0.0169 - acc: 0.9948
   Epoch 8/10
    - 1s - loss: 0.0145 - acc: 0.9956
   Epoch 9/10
    - 1s - loss: 0.0131 - acc: 0.9959                                                                                    
   Epoch 10/10                                                                                                           
    - 1s - loss: 0.0120 - acc: 0.9959                                                                                    
   INFO:root:Test Accuracy: 94.642857
   ```
   跑完这一步后，我们就得到了3个重要的文件：

   * resources/models/model.h5
   * resources/models/model.json
   * resources/vocab_tokenizer

   关于这三个文件的作用，我们稍后会有详细介绍。

4. 语言检测

   ```bash
   python -m src.detector

   Using TensorFlow backend.
   Python
   ```

   detector.py默认检测的是这段python代码：

   ```python
   def test():
       print("something")
   ```

   当然，你也可以修改detector.py，尝试让它检测其他代码。

# 代码结构

在在介绍检测原理之前，我们首先简单介绍一下代码结构，不要担心，这大概只会占用1～2分钟左右的时间，通过了解代码结构，我们可以对程序有一个大概的了解。

- 有resources/code/train：训练数据，下面所有的子文件夹的名字都是程序语言，每个子文件夹的下面又包含了10个左右的代码文件，所使用的语言就是其所在的文件夹的名字。![train文件夹结构](/images/2017-11-26-163206_448x610_scrot.png)
- resources/code/test：与`resources/code/train`代码结构相同，但与之不同的是，`test`文件夹下的代码用于评估模型的准确度，而非训练。
- `models`文件夹和`vocab_tokenizer`：训练结果。
- src/config.py：保存程序运行所需要的一些常量。
- src/neural_network_trainer.py：用于训练模型。
- src/detector.py：用于加载训练的结果并用其检测代码语言。

# 原理

## 构建语料库vocab

我们首先来讲解训练的过程，也就是neural_network_trainer.py的内容。训练该神经网络的第一步，是构造语料库vocab，语料库实际上就是一个单词列表，其中包含了训练数据中最常出现的一些单词。当我们训练好了语料库，开始检测代码语言的时候，我们首先会将代码拆分为一连串的单词，然后将不在语料库中的单词剔除掉，只保留语料库中存在的单词，然后将这些单词输入进训练好的神经网络中，进行检测。

那你可能会问了，为什么要把不在语料库中的单词给剔除掉？把全部单词都输入进神经网络中不好吗？实际上，这是一个几乎不可能完成的任务。因为语料库中的每个单词都对应一个词向量，而该词向量就是我们训练的结果，语料库之外的单词是没有词向量的，没有对应的词向量也就意味着神经网络无法处理这个单词，所以我们只能保留语料库内的词汇。

那我们如何构造语料库呢？很简单，我们扫描`resources/code/train`中的所有代码，将其中常见的单词提取出来即可，这些常见的单词就是我们的语料库，关键代码如下。

```python
def build_vocab(train_data_dir):
    vocabulary = Counter()
    files = get_files(train_data_dir)
    for f in files:
        words = load_words_from_file(f)
        vocabulary.update(words)

    # remove rare words
    min_count = 5
    vocabulary = [word for word, count in vocabulary.items() if count >= min_count]
    return vocabulary
```

运行`build_vocab`函数，我们就可以拿到语料库了。

```python
vocab = build_vocab(config.train_data_dir)
print(vocab) # [..., 'script', 'text', 'head', 'appendChild', 'parentNode', 'removeChild', ...]
```

可以看到，语料库实际上就是一个单词列表而已。

## 构建vocab_tokenizer

然后我们进行下一步，构建`vocab_tokenizer`。那什么是`vocab_tokenizer`？很简单，你可以把它想象成一个字典，它可以将我们上一步中拿到的语料库中的每一个单词都映射为一个数字。为什么要映射为数字呢？因为我们的神经网络只能根据数字进行运算，不能接受字符串。

我们可以借助Keras的Tokenizer来构建`vocab_tokenizer`。

```python
def build_vocab_tokenizer_from_set(vocab):
    vocab_tokenizer = Tokenizer(lower=False, filters="")
    vocab_tokenizer.fit_on_texts(vocab)
    return vocab_tokenizer
```

然后我们将这个`vocab_tokenizer`保存为文件，以便后续使用。

```python
def save_vocab_tokenizer(vocab_tokenzier_location, vocab_tokenizer):
    with open(vocab_tokenzier_location, 'wb') as f:
        pickle.dump(vocab_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)
```

## 构建词向量

在构建词向量之前，我们需要先了解一下什么是词向量。

简单来说，词向量就是一个向量而已，语料库中的每一个单词都有一个对应的词向量。这样说可能比较抽象，我们以下面这段Java代码为例。

```java
public static void main(String[] args) {
    System.out.println("something")
}
```

我们所构建的词向量实际上就是一个字典，大体是这个样子：

```python
word2vec = {
    'public': [2, 1, 10],
    'static': [2, 1, 9],
    'main': [1, 10, 3],
    'String': [1, 20, 3],
    'args': [1, 40, 3],
    'System': [20, 10, 3],
    'out': [3, 10, 3],
    'println': [1, 39, 3],
    'something': [1, 20, 3]
}
```

那问题来了，我们为什么要构造词向量，而不是直接使用之前`vocab_tokenizer`中所映射的数字呢？因为词向量有一个比较有用的特性：**关系越密切的单词，所对应的词向量的距离就越小**（注：向量距离的计算属于一个数学领域的范畴，有多种不同的计算方法，如果你不知道向量距离如何计算的话也没关系，只需要知道向量之间是有距离的就可以了）。这种特性会显著提高我们所训练的神经网络的准确度。

比如在Java中，`public`和`static`经常连起来一起用，所以它们所对应的词向量就很相近。而`public`和`System`的关系就不那么密切了，也就是说，这两个单词并不一定会同时出现，所以他们所对应的词向量距离就相对较远。

那知道了为什么要构造词向量，剩下的问题就是我们该如何构造它。构造词向量有很多种方式，这里我们直接调用gensim所提供的Word2Vec算法来生成词向量。大体过程如下：

1. 加载所有的训练数据，提取其中语料库所包含的的单词。
2. 通过vocab_tokenizer，将每个单词转换为对应的数字。
3. 将这些数字代入Word2Vec库中，得到词向量。

代码如下：

```python
def build_word2vec(train_data_dir, vocab_tokenizer):
    all_words = []
    files = get_files(train_data_dir)
    for f in files:
        words = load_words_from_file(f)
        all_words.append([word for word in words if is_in_vocab(word, vocab_tokenizer)])
    model = Word2Vec(all_words, size=100, window=5, workers=8, min_count=1)
    return {word: model[word] for word in model.wv.index2word}
```

## 训练神经网络

万事俱备，终于到了我们训练神经网络的时间了！我们首先说一下我们将要训练的这个神经网络的输入和输出。以下面这段代码为例：

```python
def test():
    print("something")
```

我们的神经网络的输入就是下面这四个单词所对应的数字，比如

```python
input = [0, 1, 2, 3]
```

注意input中0, 1, 2, 3分别对应def, test, print和something四个单词。

神经网络的输出就是每种语言的概率。

```python
output = [0.5, 0.1, 0.04, 0.06, 0.1, 0.1, 0.05, 0.05]
```

所对应的语言如下：

```python
all_languages = ["Python", "C", "Java", "Scala", "Javascript", "CSS", "C#", "HTML"]
```

这样我们就能推断出以上代码最有可能使用python写的，因为它的概率最高（0.5）。

知道了输入输出，下面我们来介绍一下这个神经网络的组成部分。我们的神经网络一共分为三块：

1. Embedding Layer：该层的作用是将输入给神经网络的单词转化为词向量
2. Conv1D, MaxPooling1D：这部分就是一个比较经典的卷积神经网络，简单来说，它做的就是提取和转化的操作，具体原理可参见相关卷积神经网络的教程。
3. Flatten, Dense：将之前的多维输入转化为一维输入，并输出每一种语言的概率。

关键代码如下：

```python
def build_model(train_data_dir, vocab_tokenizer, word2vec):
    weight_matrix = build_weight_matrix(vocab_tokenizer, word2vec)

    # build the embedding layer
    input_dim = len(vocab_tokenizer.word_index) + 1
    output_dim = get_word2vec_dimension(word2vec)
    x_train, y_train = load_data(train_data_dir, vocab_tokenizer)

    embedding_layer = Embedding(input_dim, output_dim, weights=[weight_matrix], input_length=input_length,
                                trainable=False)
    model = Sequential()
    model.add(embedding_layer)
    model.add(Conv1D(filters=128, kernel_size=5, activation="relu"))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(len(all_languages), activation="sigmoid"))
    logging.info(model.summary())
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(x_train, y_train, epochs=10, verbose=2)
    return model
```

这样我们就建立好了神经网络模型，接下来我们写一个函数，用这个神经网络模型检测一下测试数据，看一下准确度怎么样。

```python
def evaluate_model(test_data_dir, vocab_tokenizer, model):
    x_test, y_test = load_data(test_data_dir, vocab_tokenizer)
    loss, acc = model.evaluate(x_test, y_test, verbose=0)
    logging.info('Test Accuracy: %f' % (acc * 100))
```

之前我们已经运行过了，测试数据的准确度大概在94% ~ 95%之间，较为理想。因此我们就把这个神经网络及其相关参数保存下来，以便检测时提取使用。

```python
def save_model(model, model_file_location, weights_file_location):
    os.makedirs(os.path.dirname(model_file_location), exist_ok=True)
    with open(model_file_location, "w") as f:
        f.write(model.to_json())
    model.save_weights(weights_file_location)
```

## 调用训练好的神经网络模型进行检测

这部分比较简单，我们只需要加载训练好的`vocab_tokenizer`和神经网络，进行检测就可以了。代码如下。

```python
vocab_tokenizer = load_vocab_tokenizer(config.vocab_tokenizer_location)
model = load_model(config.model_file_location, config.weights_file_location)

def to_language(binary_list):
    i = np.argmax(binary_list)
    return all_languages[i]

def get_neural_network_input(code):
    encoded_sentence = load_encoded_sentence_from_string(code, vocab_tokenizer)
    return pad_sequences([encoded_sentence], maxlen=input_length)

def detect(code):
    y_proba = model.predict(get_neural_network_input(code))
    return to_language(y_proba)
```

使用方法如下：

```python
code = """
def test():
    print("something")
"""
print(detect(code)) # Python
```

# 总结

总体来看，构建这个神经网络需要以下步骤：

1. 构建语料库vocab。
2. 通过语料库构建`vocab_tokenizer`，用于将单词转化为数字。
3. 将所有的单词输入进Word2Vec算法中，训练词向量。
4. 将训练的词向量代入到神经网络中作为输入层的参数
5. 扫描所有的训练数据，提取其中语料库内的单词，并将每个单词通过`vocab_tokenizer`转化为数字，代入到神经网络中，进行训练。

检测的步骤分为以下3步：

1. 将代码中的单词提取出来，并去除不在语料库中的部分。
2. 通过`vocab_tokenizer`将这些单词转化为数字，并代入到神经网络中。
3. 选出神经网络输出中概率最高的那个语言，就是我们想要的答案。

# 课后习题

你可能发现了，我们在训练模型的时候，只保存了`vocab_tokenizer`和神经网络（models文件夹下），我们为什么不保存词向量word2vec和语料库vocab？

# 疑问

如果你有疑问，请在评论中留言，我会尽力给你解答。
